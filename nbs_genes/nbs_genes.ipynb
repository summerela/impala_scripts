{
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "NBS Gene Annotation"
     ]
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Purpose"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Variants were annotated with the NBS gene table provided by Ben Solomon at ITMI. Variants located in NBS gene regions were annotated with Kaviar frequency, ClinVar clinical significance ratings, DANN predicted pathogenicity scores for SNV's, and coding consequence predictions using SnpEff. \n",
      "\n",
      "Variants were filtered for appropriate mode of inheritance and then labeled as predictive when meeting the following conditions:   \n",
      "- Presence of mutation(s) with appropriate inheritance (eg, 2 bi-allelic pathogenic mutations for a recessive disorder).  \n",
      "\n",
      "Mutations defined strictly as either:   \n",
      "    - Annotated in ClinVar with a clinical significance of 4 or 5, but never 2 or 3 or labeled pathogenic in HGMD (to be added later)  \n",
      "    OR    \n",
      "    - Novel (in Kaviar with frequency less than .03 or not in Kaviar) but either predicted to be disease-causing with a SnpEff impact score of 'high' or a DANN score higher than specified cutoff below"
     ]
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "User Specified Parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enter your search paramaters below. \n",
      "\n",
      "Sample ID (sample_list)   \n",
      "\n",
      "- Enter a list of exact sample id's as comma_separated values, in the following format:  \n",
      "\n",
      "    - sample_list = '101-223-02','102-335-01'   \n",
      "- Or you can upload a txt file of sample id's, with each id on a separate line  \n",
      "- To search in all sample id's, enter 'all': \n",
      "    - sample_list = 'all'\n",
      "\n",
      "Trio member and/or wildcard searching (trio_member)  \n",
      "\n",
      "- Enter a list of family members to search,as comma_separated values, in the following format:  \n",
      "  - trio_member = 'NB','M','F'   \n",
      "- To search for all family members, including extended, enter 'all':   \n",
      "   - trio_member = 'all'  \n",
      "- You can also use this list to peform wildcard searches, for example, all trio members LIKE '101-225%':  \n",
      "    - trio_member = 'NB','M','101-225%'\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#############\n",
      "## kaviar ##\n",
      "############\n",
      "# enter kaviar frequency threshold\n",
      "kav_freq = '.03'\n",
      "\n",
      "###############\n",
      "## family id ##\n",
      "###############\n",
      "# uncomment for list \n",
      "#sample_list = '102-07005-01','102-00997-01'\n",
      "sample_list = 'all'\n",
      "\n",
      "# uncomment to read in txt file\n",
      "# samples = open('your_samples.txt', 'r').readlines()\n",
      "#sample_list = \"'\" + \"','\".join(map(str.strip, samples))+ \"'\"\n",
      "\n",
      "####################\n",
      "## trio member(s) ##\n",
      "####################\n",
      "# enter as NB, M, and/or F, or 'all' to include extended family if available\n",
      "trio_member = 'NB','M','F'\n",
      "#trio_member = 'all'\n",
      "\n",
      "################\n",
      "## dann_score ##\n",
      "################\n",
      "# enter minimum dann score for predicted NBS\n",
      "dann_score = '0.96'\n",
      "\n",
      "########################\n",
      "## database locations ##\n",
      "########################\n",
      "# enter user database to output tables\n",
      "out_db = 'users_selasady'\n",
      "# enter database.name for variant table\n",
      "variant_table = 'p7_platform.brady_variant'\n",
      "# enter database.name of kaviar table\n",
      "kaviar_table = 'public_hg19.kaviar'\n",
      "# enter database.name of clinvar table\n",
      "clinvar_table = 'public_hg19.clinvar'\n",
      "# enter database.name of dann table\n",
      "dbnfsp_table = 'p7_ref_grch37.dbnsfp_variant'\n",
      "\n",
      "#################\n",
      "## system info ##\n",
      "#################\n",
      "# enter path to snpEff.jar on your computer\n",
      "\n",
      "# windows\n",
      "snpeff_path = 'D:/Documents/tools/snpEff/snpEff.jar'\n",
      "\n",
      "# mac\n",
      "#snpeff_path = '/Users/selasady/tools/snpEff/snpEff.jar'\n",
      "\n",
      "# enter desired basename for output files\n",
      "out_name = 'nbs_genes'"
     ],
     "language": "python",
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Transform of NBS Gene Table"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The NBS Gene Table provided by ITMI was transformed as follows and uploaded to impala:   \n",
      "\n",
      "- A 'level' column was added to represent the color coding found in the file, with the following options: red, yellow, null  \n",
      "- All commas were replaced with colons to prevent errors while importing csv file  \n",
      "- All semicolons were replaced with colons for consistency of data  \n",
      "- Spaces in column names were replaced with underscores  \n",
      "- Special characters were removed from column names"
     ]
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Adding positional information to the NBS gene list"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Chromosome, start and stop positions were added to the NBS Gene list by joining it with the \n",
      "ucsc_genes table using gene name. This resulted in mulitple listings per gene for each exon, \n",
      "allowing for more precise matches. \n",
      "\n",
      "        CREATE TABLE nbs_ucsc AS (\n",
      "             SELECT DISTINCT nbs.*, ucsc.chrom, ucsc.txstart, ucsc.txend, ucsc.mrna\n",
      "             FROM nbs_genes as nbs, p7_ref_grch37.ucsc_genes as ucsc\n",
      "             WHERE nbs.gene = ucsc.gene_name\n",
      "           )\n",
      "\n",
      "The results were saved as users _ selasady.nbs _ ucsc containing 611 rows. "
     ]
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Finding rare variants with Kaviar"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For all specified trio members, variants were annotated with Kaviar, labeling variants under the Kaviar frequency threshold, or not listed in Kaviar, as rare. Variants were then joined with the nbs_ucsc table to retain only variants falling in NBS gene regions.  "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# format trio argument\n",
      "member_list = []\n",
      "for member in trio_member:\n",
      "    if member == 'NB':\n",
      "        member_list.append(\"bv.sample_id LIKE '%03'\")\n",
      "    if member == 'M':\n",
      "        member_list.append(\"bv.sample_id LIKE '%01'\")\n",
      "    if member == 'F':\n",
      "        member_list.append(\"bv.sample_id LIKE '%02'\")\n",
      "    if member_list == 'all':\n",
      "        member_list =''\n",
      "        \n",
      "# if the member argument is not empty create statement\n",
      "if len(member_list) > 0:\n",
      "    member_arg = 'AND (' + ' OR '.join(member_list) + ')'\n",
      "# otherwise statment is empty\n",
      "else: member_arg = ''\n",
      "\n",
      "# format sample id argument\n",
      "sample_arg = []\n",
      "if sample_list != 'all':\n",
      "    sample_arg.append(\"AND bv.sample_id IN \" + str(sample_list))\n",
      "    subject_list = \", \".join(str(i) for i in sample_arg)\n",
      "else: \n",
      "    subject_list = ''\n",
      "\n",
      "# list of user args to join \n",
      "arg_list = [subject_list, member_arg]\n",
      "\n",
      "# if there's an argument, format\n",
      "if len(arg_list) > 0:\n",
      "    subject_statement = ' '.join(arg_list)\n",
      "# otherwise return empty string\n",
      "else:\n",
      "    subject_statement = ''"
     ],
     "language": "python",
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# connect to impala\n",
      "from impala.dbapi import connect\n",
      "from impala.util import as_pandas\n",
      "\n",
      "# to connect to specific database, use the database argument\n",
      "conn=connect(host='glados19', port=21050)"
     ],
     "language": "python",
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running the following query was run on impala: \n\n\n    CREATE TABLE users_selasady.nbs_kaviar AS         \n          WITH bv as (\n              SELECT bv.sample_id, bv.chr as chrom, bv.pos, bv.id, bv.ref, \n                  bv.alt, bv.qual, bv.filter, regexp_replace(gt, '1/2', '1/1') as geno, k.alle_freq,  \n                  CASE WHEN k.alle_freq < .03 then 'Y'\n                  WHEN (k.alle_freq IS NULL AND (length(bv.alt) = 1)) then 'Y'\n                  ELSE 'N'\n                  END as kaviar_rare\n             FROM p7_platform.brady_variant bv\n             LEFT JOIN /* +SHUFFLE */ public_hg19.kaviar k\n                  ON bv.chr = k.chromosome\n                  AND bv.pos = k.pos\n                  AND bv.alt = k.alt\n            WHERE (bv.chr <> 'X' AND bv.chr <> 'Y')\n             AND (bv.sample_id LIKE '%03' OR bv.sample_id LIKE '%01' OR bv.sample_id LIKE '%02')\n         )\n        SELECT DISTINCT bv.*, nbs.gene, nbs.entrez_gene_id, nbs.condition, nbs.mrna,\n            nbs.omim_phenotype, nbs.va_name, nbs.inheritance, nbs.allelic_condition,nbs.in_pgb, \n            nbs.level, nbs.comments          \n       FROM bv, users_selasady.nbs_ucsc nbs\n       WHERE bv.chrom = nbs.chrom\n       AND bv.pos BETWEEN nbs.txstart and nbs.txend\n       \nQuery finished. Closing connection."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "input": [
      "# create kaviar query\n",
      "kaviar_query = '''\n",
      "\n",
      "    CREATE TABLE {}.nbs_kaviar AS         \n",
      "          WITH bv as (\n",
      "              SELECT bv.sample_id, bv.chr as chrom, bv.pos, bv.id, bv.ref, \n",
      "                  bv.alt, bv.qual, bv.filter, regexp_replace(gt, '1/2', '1/1') as geno, k.alle_freq,  \n",
      "                  CASE WHEN k.alle_freq < {} then 'Y'\n",
      "                  WHEN (k.alle_freq IS NULL AND (length(bv.alt) = 1)) then 'Y'\n",
      "                  ELSE 'N'\n",
      "                  END as kaviar_rare\n",
      "             FROM {} bv\n",
      "             LEFT JOIN /* +SHUFFLE */ {} k\n",
      "                  ON bv.chr = k.chromosome\n",
      "                  AND bv.pos = k.pos\n",
      "                  AND bv.alt = k.alt\n",
      "            WHERE (bv.chr <> 'X' AND bv.chr <> 'Y')\n",
      "            {}\n",
      "         )\n",
      "        SELECT DISTINCT bv.*, nbs.gene, nbs.entrez_gene_id, nbs.condition, nbs.mrna,\n",
      "            nbs.omim_phenotype, nbs.va_name, nbs.inheritance, nbs.allelic_condition,nbs.in_pgb, \n",
      "            nbs.level, nbs.comments          \n",
      "       FROM bv, {}.nbs_ucsc nbs\n",
      "       WHERE bv.chrom = nbs.chrom\n",
      "       AND bv.pos BETWEEN nbs.txstart and nbs.txend\n",
      "       '''.format(out_db, kav_freq, variant_table, kaviar_table, subject_statement, out_db)\n",
      "\n",
      "# open connection, run query, close connection \n",
      "def run_query(query_name, db_name):\n",
      "    # drop table if it exists\n",
      "    cur = conn.cursor()\n",
      "    cur.execute('DROP TABLE IF EXISTS {}.{}'.format(out_db,db_name))\n",
      "    cur.close()\n",
      "    # run query \n",
      "    print 'Running the following query was run on impala: \\n' + query_name\n",
      "    cur = conn.cursor()\n",
      "    cur.execute(query_name)\n",
      "    print 'Query finished. Closing connection.'\n",
      "    cur.close()\n",
      "\n",
      "# run kaviar annotation query\n",
      "run_query(kaviar_query, 'nbs_kaviar')"
     ],
     "language": "python",
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Annotating with ClinVar and DANN scores "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Variants in NBS gene regions were then annotated with the ClinVar table by genomic position. Variants with a ClinVar significance rating of 4 or 5, but never 2 or 3, were marked with a 'Y' in the clin_patho column to denote non-conflicted pathogenically significant ratings.\n",
      "\n",
      "DANN scores are provided for SNV's and range between 0 and 1. The closer a score is to 1, the more pathogenic the variant. The DANN table was matched to variants on genomic position and alternate allele. "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running the following query was run on impala: \n\n        CREATE TABLE IF NOT EXISTS users_selasady.nbs_annotated AS \n        WITH nbs as (\n             SELECT DISTINCT nbs.*, clin.clin_hgvs, clin.clin_sig, clin.clin_dbn,\n                 CASE WHEN (clin.clin_sig NOT REGEXP '3|2[^5]|2$'  \n                     AND  clin.clin_sig REGEXP '4|[^25]5|^5') THEN 'Y'\n                     ELSE 'N'\n                 END AS clin_patho\n             FROM users_selasady.nbs_kaviar nbs\n             LEFT JOIN public_hg19.clinvar clin\n                 ON nbs.chrom = clin.chrom\n                 AND nbs.pos = clin.pos\n                 AND nbs.alt = clin.alt\n          )\n\n        SELECT nbs.*, db.aa_alt, db.sift_score, db.sift_pred, db.polyphen2_hdiv_score,\n            db.polyphen2_hdiv_pred, db.polyphen2_hvar_score, db.polyphen2_hvar_pred, \n            db.fathmm_score, db.fathmm_pred, db.cadd_raw, db.dann_score, db.mutation_taster_pred,\n            db.mutation_assessor_score, mutation_assessor_pred, db.provean_score, db.provean_pred,\n            db.clinvar_clnsig, db.interpro_domain, db.exac_af,\n            CASE\n                WHEN SUBSTRING(nbs.sample_id, -2) = '01'THEN 'M'\n                WHEN SUBSTRING(nbs.sample_id, -2) = '02' THEN 'F'\n                WHEN SUBSTRING(nbs.sample_id, -2) = '03'THEN 'NB'\n                END as member,\n            SUBSTRING(nbs.sample_id, 1, (length(nbs.sample_id)-3)) as family_id,\n            CONCAT(nbs.chrom, ':', CAST(nbs.pos AS STRING), ':', nbs.ref, ':', nbs.alt) as var_id \n        FROM nbs, p7_ref_grch37.dbnsfp_variant db\n            WHERE nbs.chrom = db.chrom\n            AND nbs.pos = db.pos\n            AND nbs.alt = db.alt\n            \nQuery finished. Closing connection."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "input": [
      "dbnsfp_query = '''\n",
      "        CREATE TABLE IF NOT EXISTS {}.nbs_annotated AS \n",
      "        WITH nbs as (\n",
      "             SELECT DISTINCT nbs.*, clin.clin_hgvs, clin.clin_sig, clin.clin_dbn,\n",
      "                 CASE WHEN (clin.clin_sig NOT REGEXP '3|2[^5]|2$'  \n",
      "                     AND  clin.clin_sig REGEXP '4|[^25]5|^5') THEN 'Y'\n",
      "                     ELSE 'N'\n",
      "                 END AS clin_patho\n",
      "             FROM {}.nbs_kaviar nbs\n",
      "             LEFT JOIN {} clin\n",
      "                 ON nbs.chrom = clin.chrom\n",
      "                 AND nbs.pos = clin.pos\n",
      "                 AND nbs.alt = clin.alt\n",
      "          )\n",
      "\n",
      "        SELECT nbs.*, db.aa_alt, db.sift_score, db.sift_pred, db.polyphen2_hdiv_score,\n",
      "            db.polyphen2_hdiv_pred, db.polyphen2_hvar_score, db.polyphen2_hvar_pred, \n",
      "            db.fathmm_score, db.fathmm_pred, db.cadd_raw, db.dann_score, db.mutation_taster_pred,\n",
      "            db.mutation_assessor_score, mutation_assessor_pred, db.provean_score, db.provean_pred,\n",
      "            db.clinvar_clnsig, db.interpro_domain, db.exac_af,\n",
      "            CASE\n",
      "                WHEN SUBSTRING(nbs.sample_id, -2) = '01'THEN 'M'\n",
      "                WHEN SUBSTRING(nbs.sample_id, -2) = '02' THEN 'F'\n",
      "                WHEN SUBSTRING(nbs.sample_id, -2) = '03'THEN 'NB'\n",
      "                END as member,\n",
      "            SUBSTRING(nbs.sample_id, 1, (length(nbs.sample_id)-3)) as family_id,\n",
      "            CONCAT(nbs.chrom, ':', CAST(nbs.pos AS STRING), ':', nbs.ref, ':', nbs.alt) as var_id \n",
      "        FROM nbs, {} db\n",
      "            WHERE nbs.chrom = db.chrom\n",
      "            AND nbs.pos = db.pos\n",
      "            AND nbs.alt = db.alt\n",
      "            '''.format(out_db, out_db, clinvar_table, dbnfsp_table)\n",
      "\n",
      "# run dbNFSP query on impala\n",
      "run_query(dbnsfp_query, 'nbs_annotated')"
     ],
     "language": "python",
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Reading the impala results into python"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#################\n",
      "# load modules  #\n",
      "#################\n",
      "from impala.util import as_pandas\n",
      "\n",
      "# query to left join nbs_annotated variants with DANN scores\n",
      "nbs_query = \"\"\"\n",
      "    SELECT * FROM {}.nbs_annotated\n",
      "    \"\"\".format(out_db)\n",
      "# run query on impala\n",
      "cur = conn.cursor()\n",
      "cur.execute(nbs_query)\n",
      "\n",
      "# store results as pandas data frame\n",
      "nbs_df = as_pandas(cur)\n",
      "cur.close()"
     ],
     "language": "python",
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11457 variants located in NBS regions.\n"
       ]
      }
     ],
     "input": [
      "if len(nbs_df) > 0:\n",
      "    print str(len(nbs_df)) + ' variants located in NBS regions.'\n",
      "else: \n",
      "    print \"No variants located in NBS regions.\""
     ],
     "language": "python",
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Adding Functional Annotation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The newborn set was output to a modified version of Plink's tab2vcf script to create a VCF file for use with SnpEff. The newborn variants in the VCF file were annotated with coding consequences using SnpEff version 4.1h (build 2015-08-03) with GRCh37.75. "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import subprocess as subp\n",
      "import os\n",
      "\n",
      "# rename 'geno' to 'gt' for vcf output\n",
      "nbs_df = nbs_df.rename(columns = {'geno':'gt'})\n",
      "\n",
      "# function to add functional annotation\n",
      "def df_to_snpeff(input_df):\n",
      "    if os.path.exists(snpeff_path):\n",
      "        # setup file names\n",
      "        tsv_outname = '{}.tsv'.format(os.path.join(os.getcwd(), out_name))\n",
      "        vcf_outname = '{}.vcf'.format(os.path.join(os.getcwd(), out_name))\n",
      "        # these columns are output to vcf file\n",
      "        df = input_df[['chrom', 'pos', 'ref', 'alt', 'qual', 'filter', 'gt']]\n",
      "        # write to file for conversion to vcf\n",
      "        df.to_csv(tsv_outname, header=True, encoding='utf-8', sep=\"\\t\", index=False)\n",
      "        # run tab2vcf and upon success, run snpeff\n",
      "        vcf_process = subp.Popen(['python', 'tab2vcf.py', tsv_outname])\n",
      "        vcf_process.wait()\n",
      "        # command to run snpeff \n",
      "        snpeff_cmd = 'java -Xmx4g -jar {} -v GRCh37.75 {} > nbs_snpeff.vcf'.format(snpeff_path, vcf_outname)\n",
      "        # run snpeff on vcf file\n",
      "        snpeff_process = subp.Popen(snpeff_cmd, shell=True)\n",
      "        snpeff_process.wait()\n",
      "    else:\n",
      "        print \"Make sure you entered the correct path to snpEff.jar\"\n",
      "\n",
      "# run function on query results\n",
      "try:\n",
      "    df_to_snpeff(nbs_df)\n",
      "except Exception, e: \n",
      "    print str(e)"
     ],
     "language": "python",
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The functional annotations were read back into Python, parsed and matched with the each respective variant data frame. "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# function to drop extra columns ending in _y from df merge\n",
      "def drop_y(df):\n",
      "    for col in df:\n",
      "        if col.endswith('_y'):\n",
      "            df.drop(col, axis=1, inplace=True)\n",
      "\n",
      "# add snpeff annotations back to dataframe\n",
      "def parse_snpeff(input_df):\n",
      "    # merge snpeff annotated vcf file with dataframe , skipping header\n",
      "    annot_vcf = pd.read_csv('nbs_snpeff.vcf', sep='\\t', skiprows=8)\n",
      "    # split info columns by \"|\" \n",
      "    info_df = pd.DataFrame(list(annot_vcf['INFO'].str.split('|')))\n",
      "    # keep only first (most detrimental) consequence listed\n",
      "    info_df = info_df[list(info_df.columns[1:11])]\n",
      "    # merge truncated info field with annot_vcf\n",
      "    annot_vcf = annot_vcf[np.r_[0,1,2, 3:7, 9]]\n",
      "    annot_df = pd.concat([annot_vcf, info_df], axis=1)\n",
      "    # rename columns\n",
      "    annot_df.columns =['chrom', 'pos', 'id', 'ref', 'alt', 'qual', 'filter', 'gt', 'effect', 'impact', 'gene_name', \n",
      "    'gene_id', 'feature_type', 'feature_id', 'tx_biotype', 'rank', 'hgvs_c', 'hgvs_p']\n",
      "    # drop unnecessary columns before merging\n",
      "    annot_df.drop(['qual', 'filter', 'gene_name', 'gene_id', 'feature_id', 'feature_type', 'rank'], axis=1, inplace=True)\n",
      "    # merge annotations with variant table\n",
      "    df_functional = pd.merge(input_df, annot_df, left_on=['var_id'], right_on=['id'], how='left')\n",
      "    drop_y(df_functional)\n",
      "    # rename columns ending in _x from merge\n",
      "    df_functional.rename(columns=lambda x: x.replace('_x', ''), inplace=True)\n",
      "    return df_functional\n",
      "    \n",
      "# merge nbs_genes with functional annotations\n",
      "nbs_annot = parse_snpeff(nbs_df)"
     ],
     "language": "python",
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Labeling potentially pathogenic mutations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "sift_pred = D\n",
      "polyphen2_hdiv_pred\n",
      "polyphen2_hvar_pred\n",
      "mutation_taster_pred\n",
      "mutation_assessor_pred\n",
      "fathmm_pred = D\n",
      "provean_pred = D\n",
      "cadd_raw\n",
      "dann_score > .96\n",
      "exac_af < .03"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# label variants considered pathogenic \n",
      "nbs_annot['predictive'] =  ((nbs_annot['kaviar_rare'] == 'rare') & ((nbs_annot['impact'] == 'HIGH') | (nbs_annot['dann_score'] >= dann_score))) | (nbs_annot['clin_patho'] == 'Y')"
     ],
     "language": "python",
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "nbs_annot.to_csv('./nbs_testing.csv', header=True, encoding='utf-8', index=False)"
     ],
     "language": "python",
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataframe was separated into mother, father and newborn for downstream analysis. "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of newborn variants: 428625\nNumber of mother variants: 384656\nNumber of father variants: 380544\n"
       ]
      }
     ],
     "input": [
      "# subset data frame by trio member\n",
      "newborns = nbs_annot[nbs_annot['member'] == 'NB']\n",
      "mothers = nbs_annot[nbs_annot['member'] == 'M']\n",
      "fathers = nbs_annot[nbs_annot['member'] == 'F']\n",
      "\n",
      "print \"Number of newborn variants: \" + str(len(newborns)) \n",
      "print \"Number of mother variants: \" + str(len(mothers)) \n",
      "print \"Number of father variants: \" + str(len(fathers))"
     ],
     "language": "python",
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The newborn subset was split to report:   \n",
      "- All variants in regions of dominant disorders  \n",
      "- All homozygous recessive variants in regions of autosomal recessive disorders  \n",
      "- All heterozygous variants in autosomal recessive regions for downstream analysis of compound heterozygosity  "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n"
       ]
      }
     ],
     "input": [
      "# disable erroneous pandas warning\n",
      "pd.options.mode.chained_assignment = None\n",
      "\n",
      "# subset variants by variant MOI and/or zygosity\n",
      "nb_dominant = newborns[((newborns['inheritance'] == 'AD') & (newborns['predictive'] == True))]\n",
      "nb_dominant.name = 'dominant'\n",
      "\n",
      "print len(nb_dominant)\n",
      "\n",
      "# nb_hom_recessive = newborns[((newborns['inheritance'] == 'AR') & (newborns['gt'] == '1/1'))]\n",
      "# nb_hom_recessive.name = 'hom_recessive'\n",
      "# \n",
      "# nb_het = newborns[((newborns['inheritance'] == 'AR') & (newborns['gt'] == '0/1'))]"
     ],
     "language": "python",
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Reporting compound heterozygous recessive variants"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Heterozygous variants in autosomal recessive regions were examined for compound heterozygosity, as follows:   \n",
      "- Newborns must have variants at more than one position on the same gene, with each variant being inherited from a het parent\n",
      "\n",
      "Newborn het variants were subset for only variants inherited from het parents, and grouped by gene: \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# keep only heterozygous parent variants, as homozygous parents are not of interest\n",
      "mom_het = mothers.loc[mothers['gt'] == '0/1']\n",
      "dad_het = fathers.loc[fathers['gt'] == '0/1']\n",
      "           \n",
      "# function to find matching parent variants\n",
      "def find_parent_vars(nb_df, parent_df):\n",
      "    # merge dataframes on variant id\n",
      "    merged_df = pd.merge(nb_df, parent_df, on=['var_id'], how='inner')\n",
      "    # rename parent sample_id column to avoid dropping when removing '_y' cols\n",
      "    merged_df.rename(columns = {'member_y':'from_parent'}, inplace=True)\n",
      "    # drop extra y columns from merge with fathers\n",
      "    drop_y(merged_df)\n",
      "    #remove _x from colnames\n",
      "    merged_df.rename(columns=lambda x: x.replace('_x', ''), inplace=True)\n",
      "    return merged_df\n",
      "    \n",
      "# run function for each parent set\n",
      "if (len(mom_het) > 0) and (len(dad_het) > 0):\n",
      "    nb_and_mom = find_parent_vars(nb_het, mom_het)\n",
      "    nb_and_dad = find_parent_vars(nb_het, dad_het)\n",
      "    # merge variants found in either mother or father\n",
      "    het_cands = pd.concat([nb_and_mom,nb_and_dad]).drop_duplicates().reset_index(drop=True)\n",
      "    # group variants by gene name\n",
      "    by_gene = het_cands.groupby(['gene'])\n",
      "else:\n",
      "    print \"No compound het variants\""
     ],
     "language": "python",
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After grouping the variants by gene, the variants will be filtered to keep only variants with at least one different variant coming from the mother and one from the father.  "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# function to find compound hets\n",
      "def find_comphets(gene_group, comphet_list_name):\n",
      "    for name, group in gene_group:\n",
      "        # if there is a variant in more than one position\n",
      "        if group.pos.nunique() > 1:\n",
      "            # and there are more than one variants from both parents\n",
      "            if len(group[group['from_parent'] == 'M'] > 1) and len(group[group['from_parent'] == 'F'] > 1):\n",
      "                comphet_list_name.append(group)\n",
      "            # or if there is only one variant from each parent\n",
      "            elif len(group[group['from_parent'] == 'M'] == 1) and len(group[group['from_parent'] == 'F'] == 1):\n",
      "                # and those variants are different\n",
      "                if len(group[group['from_parent'] == 'M'].pos - group[group['from_parent'] == 'F']) > 0:\n",
      "                        comphet_list_name.append(group)\n",
      "\n",
      "# create empty list to store comp_hets\n",
      "comp_hets = []\n",
      "\n",
      "if len(by_gene) > 0:\n",
      "    # run function on by_gene\n",
      "    find_comphets(by_gene, comp_hets)\n",
      "    # combine results into dataframe\n",
      "    comphet_df = pd.concat(comp_hets)\n",
      "    comphet_df.name = 'comp_het'\n",
      "else:\n",
      "    print 'No compound het variants found.'\n",
      "    comphet_df = pd.DataFrame()\n",
      "    comphet_df.name = 'comp_het'"
     ],
     "language": "python",
     "prompt_number": 80
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Finding newborns predicted to have NBS conditions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dominant, homozgous recessive and compound heterozygous recessive candidate variants were filtered to retain newborns predicted to have NBS conditions:     \n",
      "\n",
      "- Presence of mutation(s) with appropriate inheritance using filtering procedures outlined above   \n",
      "- Mutations defined strictly as either:   \n",
      "    - Clearly annotated in ClinVar, HGMD or both, as pathogenic or disease-causing for the NBS condition   \n",
      "     OR  \n",
      "    - Novel but predicted to be disease-causing (ie, a rating of 'HIGH' from SnpEff impact score)  \n",
      "- Additionally, compound heterozygous variants will be filtered to include only mutations where both variants inherited from the parent are pathogenic  "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# return only variants that are pathogenic\n",
      "nb_dom_patho= pd.DataFrame(nb_dominant.loc[nb_dominant['pathogenic'] == True])\n",
      "nb_dom_patho.name = 'dominant'\n",
      "nb_hom_rec_patho = pd.DataFrame(nb_hom_recessive.loc[nb_hom_recessive['pathogenic'] == True])\n",
      "nb_hom_rec_patho.name = 'hom_rec'\n",
      "\n",
      "###############################################################\n",
      "## find comp_het variants where both variants are pathogenic ##\n",
      "###############################################################\n",
      "if len(comphet_df) > 0:\n",
      "    # group comp_hets by gene and family id\n",
      "    patho_by_gene = comphet_df.groupby(['gene','family_id'])\n",
      "    # create list to store results\n",
      "    patho_comphet = []\n",
      "    # search within each gene and family:\n",
      "    for name, group in patho_by_gene:\n",
      "        # if there are pathogenic genes at more than one position\n",
      "        if (group[(group['pathogenic'] == True)].pos.nunique() > 1):\n",
      "            # append to patho_comphet\n",
      "            patho_comphet.append(group)\n",
      "            # transform results into dataframe and name it\n",
      "            patho_comphet_df = pd.concat(patho_comphet)\n",
      "            patho_comphet_df.name = 'comp_het'\n",
      "        else:\n",
      "             patho_comphet_df = pd.DataFrame()\n",
      "             patho_comphet_df.name = 'comp_het'\n",
      "else:\n",
      "    patho_comphet_df = pd.DataFrame()\n",
      "    patho_comphet_df.name = 'comp_het'"
     ],
     "language": "python",
     "prompt_number": 81
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Results"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Variants found:\n405 dominant variants found.\n1914 hom_recessive variants found.\n529 comp_het variants found.\n\n\nOf those variants, the following were predicted to cause NBS disorders:\n48 dominant variants found.\n16 hom_rec variants found.\nNo comp_het variants found.\n\ndominant:  \n\nImpact: \nMODERATE    36\nLOW          8\nMODIFIER     4\ndtype: int64 \n\nEffect: \nmissense_variant       36\nsynonymous_variant      8\n5_prime_UTR_variant     4\ndtype: int64 \n\nCondition: \nHypothyroidism: congenital: nongoitrous 2                       18\nPallister-Hall syndrome 2                                       18\nHypothyroidism: congenital nongoitrous: 5                        8\nChoreoathetosis: hypothyroidism: and neonatal respiratory distress     4\ndtype: int64 \n\nAffected gene(s): \n['NKX2-5' 'NKX2-1' 'PAX8' 'GLI2'] \n\n\nhom_rec:  \n\nImpact: \nMODIFIER    16\ndtype: int64 \n\nEffect: \n5_prime_UTR_variant    16\ndtype: int64 \n\nCondition: \nPhenylketonuria: Hyperphenylalaninemia: non-PKU mild    16\ndtype: int64 \n\nAffected gene(s): \n['PAH'] \n\n"
       ]
      }
     ],
     "input": [
      "# report variant counts\n",
      "def report_result_counts(results_df):\n",
      "    if len(results_df) > 0:\n",
      "        print str(len(results_df)) + ' {} variants found.'.format(results_df.name)\n",
      "    else:\n",
      "         print \"No {} variants found.\".format(results_df.name)\n",
      "        \n",
      "print \"Variants found:\"\n",
      "report_result_counts(nb_dominant)\n",
      "report_result_counts(nb_hom_recessive)\n",
      "report_result_counts(comphet_df)\n",
      "print \"\\n\"\n",
      "\n",
      "print \"Of those variants, the following were predicted to cause NBS disorders:\"        \n",
      "report_result_counts(nb_dom_patho)\n",
      "report_result_counts(nb_hom_rec_patho)\n",
      "report_result_counts(patho_comphet_df)\n",
      "\n",
      "def summarize(df):\n",
      "    if len(df) > 0:\n",
      "        print '\\n' + df.name + ': ', '\\n'\n",
      "        impact = df['impact'].value_counts()\n",
      "        effect = df['effect'].value_counts()\n",
      "        condition = df['condition'].value_counts()\n",
      "        genes = df['gene'].unique()\n",
      "        print 'Impact: \\n', impact, '\\n'\n",
      "        print 'Effect: \\n', effect, '\\n'\n",
      "        print 'Condition: \\n', condition, '\\n'\n",
      "        print 'Affected gene(s): \\n', genes, '\\n'\n",
      "\n",
      "summarize(nb_dom_patho)\n",
      "summarize(nb_hom_rec_patho)\n",
      "summarize(patho_comphet_df)"
     ],
     "language": "python",
     "prompt_number": 82
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Saving Output"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Saving 529 comp_het variants to current working directory\nSaving 405 dominant variants to current working directory\nSaving 1914 hom_recessive variants to current working directory"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "input": [
      "# save output to current working directory\n",
      "def save_results(df):\n",
      "    if len(df) > 0:\n",
      "        print \"Saving {} {} variants to current working directory\".format(len(df), df.name)\n",
      "        df.to_csv('{}.csv'.format(df.name), header=True, encoding='utf-8', index=False)\n",
      "        \n",
      "# save unfiltered variants to file\n",
      "save_results(comphet_df)\n",
      "save_results(nb_dominant)\n",
      "save_results(nb_hom_recessive)"
     ],
     "language": "python",
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Saving 48 dominant variants to current working directory\nSaving 16 hom_rec variants to current working directory\nNo comp_het variants to save.\n"
       ]
      }
     ],
     "input": [
      "# merge and mark predicted variants\n",
      "def merge_predictors(df, out_list):\n",
      "    if len(df) > 0:\n",
      "        df['var_type'] = df.name\n",
      "        out_list.append(df)\n",
      "        print \"Saving {} {} variants to current working directory\".format(len(df), df.name)\n",
      "    else:\n",
      "        print \"No {} variants to save.\".format(df.name)\n",
      "\n",
      "# list to store patho output\n",
      "predict_list = []            \n",
      "\n",
      "merge_predictors(nb_dom_patho, predict_list)\n",
      "merge_predictors(nb_hom_rec_patho, predict_list)\n",
      "merge_predictors(patho_comphet_df, predict_list)\n",
      "\n",
      "merged_patho = pd.concat(predict_list)\n",
      "merged_patho.to_csv('predicted_NBS.csv', header=True, encoding='utf-8', index=False)"
     ],
     "language": "python",
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      ""
     ],
     "language": "python"
    }
   ]
  }
 ],
 "cells": [],
 "metadata": {},
 "nbformat": 3,
 "nbformat_minor": 0
}