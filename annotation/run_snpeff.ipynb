{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SnpEff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genetic variant annotation and effect prediction toolbox. It annotates and predicts the effects of variants on genes (such as amino acid changes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Run VCF Through SnpEff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires: \n",
    "- SnpEff: http://snpeff.sourceforge.net/\n",
    "- Reference (downloaded automoatically) GRCh37.74 to match vcf files\n",
    "- GATK: to validate vcf file https://www.broadinstitute.org/gatk/download/\n",
    "- 1000 genomes hg37 reference fasta: ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz  \n",
    "- Matching index file ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.fai\n",
    "- Mathching dict file (created using Picard tools) \n",
    "- Ibis: 'pip install ibis-framework' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Invalid or corrupt jarfile D:/Documents/tools/picard/src/java/picard/sam/CreateSequenceDictionary.java\n"
     ]
    }
   ],
   "source": [
    "! java -jar D:/Documents/tools/picard/src/java/picard/sam/CreateSequenceDictionary.java \\\n",
    "    R=D:/Documents/GitHub/impala_scripts/annotation/human_g1k_v37.fasta \\\n",
    "    O=D:/Documents/GitHub/impala_scripts/annotation/human_g1k_v37.dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VCF from distinct kaviar variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each distinct variant found in ISB's Kaivar table, a vcf file will be created and run through SnpEff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Impala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HDFS connection, update user argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ibis\n",
    "import os\n",
    "\n",
    "# connect to impala with ibis\n",
    "hdfs_port = os.environ.get('glados20', 50070)\n",
    "hdfs = ibis.hdfs_connect(host='glados20', port=hdfs_port, user='selasady')\n",
    "con = ibis.impala.connect(host='glados19', port=21050, timeout=120)\n",
    "\n",
    "# enable interactive mode\n",
    "ibis.options.interactive = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Distinct Variant Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline begins with the table of global distcint variants in impala. Here we are downloading them into an ibis table object to output to vcf and run through snpeff for predicting coding consequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vcf(tbl_name, db_name):\n",
    "    # create ibis object from distinct vars table\n",
    "    distinct_vars = con.table(tbl_name, database=db_name) \n",
    "    # limit table to just the columns we need to output to vcf\n",
    "    distinct_df = distinct_vars['chrom', 'pos', 'ref', 'alt']\n",
    "    # download table from ibis table connection object to local memory\n",
    "    distinct_df = distinct_df.execute(limit=100000000000)\n",
    "    # add blank fields for vcf format\n",
    "    distinct_df['id'] = '.'\n",
    "    distinct_df['QUAL'] = '30'\n",
    "    distinct_df['FILTER'] = 'PASS'\n",
    "    distinct_df['INFO'] = '.'\n",
    "    distinct_df['FORMAT'] = 'GT:'\n",
    "    distinct_df['subject'] = './.'\n",
    "    # rename chrom column to match vcf header\n",
    "    distinct_df =distinct_df.rename(columns = {'CHROM':'#CHROM'})\n",
    "    # uppercase column names to match vcf header\n",
    "    distinct_df.columns = [x.upper() for x in distinct_df.columns]\n",
    "    # remove duplicated rows\n",
    "    distinct_df.drop_duplicates(inplace=True)\n",
    "    return distinct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download table from ibis table connection object to local memory\n",
    "distinct_df = create_vcf('global_vars', 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM        POS  REF  ALT ID QUAL FILTER INFO FORMAT SUBJECT\n",
      "0    20    5918258    T    C  .   30   PASS    .    GT:     ./.\n",
      "1    15   28356557    G   GA  .   30   PASS    .    GT:     ./.\n",
      "2     2  187867377  CAG    C  .   30   PASS    .    GT:     ./.\n",
      "3    11  108158205   TC    T  .   30   PASS    .    GT:     ./.\n",
      "4     1   74837216    T  TAG  .   30   PASS    .    GT:     ./.\n",
      "\n",
      " Distinct rows = 53065\n"
     ]
    }
   ],
   "source": [
    "print distinct_df.head(5) \n",
    "print \"\\n Distinct rows = \" + str(len(distinct_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Distinct Variants as VCF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# disable extraneous pandas warning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create vcf header\n",
    "def create_header(outfile_name):\n",
    "   # create vcf header\n",
    "    lines=[]\n",
    "    lines.append('##fileformat=VCFv4.0')\n",
    "    lines.append('##fileDate='+ time.strftime(\"%y%m%d\"))\n",
    "    lines.append('##reference=grch37 v.74 \\n')\n",
    "    header = '\\n'.join(lines)\n",
    "    out = open(outfile_name, 'wb')\n",
    "    out.write(header)\n",
    "    out.close()\n",
    "    \n",
    "# create vcf body and append to file with header\n",
    "def impala_to_vcf(input_df, outfile_name):\n",
    "    # add blank columns for vcf format and format col names\n",
    "    input_df.columns = [x.upper() for x in input_df.columns]\n",
    "    input_df= input_df.rename(columns = {'CHROM':'#CHROM'})    \n",
    "    # order chromosomes to match ref fastas\n",
    "    chroms = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'M', 'MT']\n",
    "    input_df['#CHROM'] = input_df['#CHROM'].astype(\"category\")\n",
    "    input_df['#CHROM'].cat.set_categories(chroms, inplace=True)\n",
    "    # sort file by chrom then pos\n",
    "    input_df = input_df.sort(['#CHROM', 'POS'])\n",
    "    # write to file for conversion to vcf\n",
    "    input_df.to_csv(outfile_name, header=True, encoding='utf-8', sep=\"\\t\", index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:23: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "vcf_out = 'distinct_vars.vcf'\n",
    "\n",
    "create_header(vcf_out)\n",
    "impala_to_vcf(distinct_df, vcf_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify VCF format using GATK ValidateVariants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GATK's validate variants method is used to verify only the VCF formatting with the --validationTypeToExclude ALL to avoid checks for allele count, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  20:22:29,559 HelpFormatter - --------------------------------------------------------------------------------- \n",
      "INFO  20:22:29,562 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.4-46-gbc02625, Compiled 2015/07/09 17:38:12 \n",
      "INFO  20:22:29,562 HelpFormatter - Copyright (c) 2010 The Broad Institute \n",
      "INFO  20:22:29,562 HelpFormatter - For support and documentation go to http://www.broadinstitute.org/gatk \n",
      "INFO  20:22:29,566 HelpFormatter - Program Args: -T ValidateVariants -R D:/Documents/GitHub/impala_scripts/annotation/human_g1k_v37.fasta -V distinct_vars.vcf --validationTypeToExclude ALL \n",
      "INFO  20:22:29,575 HelpFormatter - Executing as Summer@Beast on Windows 8.1 6.3 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_51-b16. \n",
      "INFO  20:22:29,576 HelpFormatter - Date/Time: 2015/11/02 20:22:29 \n",
      "INFO  20:22:29,576 HelpFormatter - --------------------------------------------------------------------------------- \n",
      "INFO  20:22:29,576 HelpFormatter - --------------------------------------------------------------------------------- \n",
      "INFO  20:22:30,107 GenomeAnalysisEngine - Strictness is SILENT \n",
      "INFO  20:22:40,481 GATKRunReport - Uploaded run statistics report to AWS S3 \n",
      "##### ERROR ------------------------------------------------------------------------------------------\n",
      "\n",
      "##### ERROR A USER ERROR has occurred (version 3.4-46-gbc02625): \n",
      "\n",
      "##### ERROR\n",
      "##### ERROR This means that one or more arguments or inputs in your command are incorrect.\n",
      "\n",
      "##### ERROR The error message below tells you what is the problem.\n",
      "\n",
      "##### ERROR\n",
      "##### ERROR If the problem is an invalid argument, please check the online documentation guide\n",
      "\n",
      "##### ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.\n",
      "\n",
      "##### ERROR\n",
      "##### ERROR Visit our website and forum for extensive documentation and answers to \n",
      "\n",
      "##### ERROR commonly asked questions http://www.broadinstitute.org/gatk\n",
      "\n",
      "##### ERROR\n",
      "##### ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.\n",
      "\n",
      "##### ERROR\n",
      "##### ERROR MESSAGE: Fasta dict file D:\\Documents\\GitHub\\impala_scripts\\annotation\\human_g1k_v37.dict for reference D:\\Documents\\GitHub\\impala_scripts\\annotation\\human_g1k_v37.fasta does not exist. Please see http://gatkforums.broadinstitute.org/discussion/1601/how-can-i-prepare-a-fasta-file-to-use-as-reference for help creating it.\n",
      "\n",
      "##### ERROR ------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!java -Xmx16g -jar D:/Documents/tools/GenomeAnalysisTK.jar  -T ValidateVariants \\\n",
    "    -R D:/Documents/GitHub/impala_scripts/annotation/human_g1k_v37.fasta \\\n",
    "        -V distinct_vars.vcf --validationTypeToExclude ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run VCF File through SnpEff to predict coding consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running SnpEff on output vcf file: \n",
    "    - -Xmx16g : specify amount of java memory \n",
    "    - -t : multi-threaded mode, cannot spedify threads, determines automatically\n",
    "    - -v : verbose mode\n",
    "    - -noStats : save time by not calculating stats on the variants found\n",
    "    - GRCh37.74 : specifies reference database to use, matched with VCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# command to run snpeff on mac\n",
    "#!java -Xmx16g -jar /Users/selasady/tools/snpEff/snpEff.jar -t -v -noStats GRCh37.74 distinct_vars.vcf > distinct_snpeff.vcf\n",
    "\n",
    "# command to run snpeff windows\n",
    "!java -Xmx16g -jar D:/Documents/tools/snpEff/snpEff.jar -t -noStats GRCh37.74 distinct_vars.vcf > distinct_snpeff.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output SnpEff effects as tsv file, one effect per line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the snpeff from vcf format to tsv, with one line per effect, instead of grouped with multiple effects for each position. \n",
    "\n",
    "NOTE: vcfEffOnePerLine.pl does not work properly in older verions of SnpEff, make sure you have downloaded the latest version. The perl script is containted within the /snpeff/scripts directory that comes with snpeff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-90aeeedd9941>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-90aeeedd9941>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    cat distinct_snpeff.vcf     | /Users/selasady/tools/snpEff/scripts/vcfEffOnePerLine.pl     | java -jar /Users/selasady/tools/snpEff/SnpSift.jar extractFields     - CHROM POS REF ALT \"ANN[*].GENE\" \"ANN[*].GENEID\" \"ANN[*].EFFECT\" \"ANN[*].IMPACT\"     \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \"ANN[*].BIOTYPE\" \"ANN[*].RANK\"     \"ANN[*].HGVS_C\" \"ANN[*].HGVS_P\" > distinct.tsv\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cat distinct_snpeff.vcf \\\n",
    "    | /Users/selasady/tools/snpEff/scripts/vcfEffOnePerLine.pl \\\n",
    "    | java -jar /Users/selasady/tools/snpEff/SnpSift.jar extractFields \\\n",
    "    - CHROM POS REF ALT \"ANN[*].GENE\" \"ANN[*].GENEID\" \"ANN[*].EFFECT\" \"ANN[*].IMPACT\" \\\n",
    "    \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \\\n",
    "    \"ANN[*].HGVS_C\" \"ANN[*].HGVS_P\" > distinct.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Header and convert to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for upload to HDFS and conversion into impala table, the header will be removed so we can specify our own column names in the schema below and parse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sed '1d' distinct.tsv | tr '\\t' ',' > distinct_snpeff.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results as an Impala Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Results to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV file is uploaded to HDFS, in a directory called 'snpeff'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:requests.packages.urllib3.connectionpool:Connection pool is full, discarding connection: glados13.systemsbiology.net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/user/selasady/training/distinct_snpeff.csv'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/user/selasady/training/'\n",
    "\n",
    "# create directory\n",
    "hdfs.mkdir(path)\n",
    "\n",
    "# upload file\n",
    "file_name = 'distinct_snpeff.csv'\n",
    "admix_file = path + '/' + file_name\n",
    "\n",
    "# upload admix file\n",
    "hdfs.put(path, file_name, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert TSV into Ibis Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the TSV file is in HDFS, we can use ibis to convert it into an impala table by pairing the file with a schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define table schema for tsv file\n",
    "schema = ibis.schema([\n",
    "    ('chrom', 'string'), \n",
    "    ('pos', 'int32'),\n",
    "    ('ref', 'string'),\n",
    "    ('alt', 'string'),\n",
    "    ('gene', 'string'),\n",
    "    ('gene_id', 'string'),\n",
    "    ('effect', 'string'),\n",
    "    ('impact', 'string'),\n",
    "    ('feature', 'string'),\n",
    "    ('feature_id', 'string'),\n",
    "    ('biotype', 'string'),\n",
    "    ('rank', 'int32'),\n",
    "    ('hgvs_c', 'string'),\n",
    "    ('hgvs_p', 'string')\n",
    "])\n",
    "\n",
    "# create ibis object from  tsv\n",
    "snpeff = con.delimited_file(path, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  chrom     pos ref alt gene gene_id effect impact feature feature_id biotype  \\\n",
      "0     1  949696  CG   .                                                         \n",
      "1     1  949696  CG   .                                                         \n",
      "2     1  949739   T   .                                                         \n",
      "3     1  949739   T   .                                                         \n",
      "4     1  985955   C   .                                                         \n",
      "\n",
      "   rank hgvs_c hgvs_p  \n",
      "0  None                \n",
      "1  None                \n",
      "2  None                \n",
      "3  None                \n",
      "4  None                \n"
     ]
    }
   ],
   "source": [
    "print snpeff.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create impala table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the TSV file is paired with a schema, we can save it as a table in impala. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con.create_table('coding_consequences', snpeff, database='p7_ref_grch37')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
